# Data Modeling With Postgres

## Introduction

A startup called `Sparkify` wants to analyze the data they've been collecting on songs and user activity on their new music streaming app. The analytics team is particularly interested in understanding what songs users are listening to. Currently, they don't have an easy way to query their data, which resides in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

They'd like a data engineer to create a Postgres database with tables designed to optimize queries on song play analysis, and bring you on the project. Your role is to create a database schema and ETL pipeline for this analysis. You'll be able to test your database and ETL pipeline by running queries given to you by the analytics team from Sparkify and compare your results with their expected results.

## Project Description

In this project, you'll apply what you've learned on data modeling with `Postgres` and build an ETL pipeline using `Python`. To complete the project, you will need to define fact and dimension tables for a star schema for a particular analytic focus, and write an ETL pipeline that transfers data from files in two local directories into these tables in `Postgres` using `Python` and `SQL`.

## Data Sources

### Song Dataset

The first dataset is a subset of real data from the [Million Song Dataset](http://millionsongdataset.com/). Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset.

```
song_data/A/B/C/TRABCEI128F424C983.json
song_data/A/A/B/TRAABJL12903CDCF1A.json
```

And below is an example of what a single song file, TRAABJL12903CDCF1A.json, looks like.

`{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}`

### Log Dataset

The second dataset consists of log files in JSON format generated by this [event simulator](https://github.com/Interana/eventsim) based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.

The log files in the dataset you'll be working with are partitioned by year and month. For example, here are filepaths to two files in this dataset.

```
log_data/2018/11/2018-11-12-events.json
log_data/2018/11/2018-11-13-events.json
```

And below is an example of what the data in a log file, 2018-11-12-events.json, looks like.

![alt text](/img/log-data.png)


If you would like to look at the JSON data within log_data files, you will need to create a pandas dataframe to read the data. Remember to first import JSON and pandas libraries.

```
df = pd.read_json(filepath, lines=True)
```

For example, `df = pd.read_json('data/log_data/2018/11/2018-11-01-events.json', lines=True)` would read the data file 2018-11-01-events.json.

## Discussion

### Purpose of this database in the context of the startup, Sparkify

First, the database is necessary to have a better way to store data coming from the product than files system. The data are now properly store with less risk to lose it or to be corrupted.

The second main purpose is to be able to access the data properly in a comprehensive way for all the employees. The database schema is described and documented so that each one in the company is able to understand where are the data and how to access it if needed for business purpose.

And the data access is now clear and easy for everyone. Search in a particular file system without documentation is not easy and data can be easily corrupted, lost or changed by anyone. With the Postgres database, you can have different level of access in the database (users) and restricted rights on it. And having a clean and easy data access is the first step before running any analytics on the data.

### Sparkify analytic goals

The database set up, the startup can now run some analytics for business prupose.

Like for example how many users are using the application with a free access, how many time each user is using the application per day, month or year. And even understand some key point on the users (like age, location, songs they are listening). For marketing purpose, a database is help a lot to understand customers behaviors and customers profils.

Moreover, to build new services like recommendation engine based on users experience, it is mandatory to have this database.

### Database schema

The exercice recommend a star schema for the database. The fact table is the `songplays` table recording event coming from the application: when a user is playing a new song, record which user, which song and when.

Then it stores useful informations on users in the `users` table (name, location, subscription level, ...), on songs in the `songs` table (title, year, artist ID, duration, ...), on artists in the `artists` table (name, location, ...), and a table `time` to have an easier treatment of records timestamps.

`songplays`, `time` and `users` tables are populated with logs data coming from the application developped by Sparkify.

`songs` and `artists` tables are populated with the data coming from song data with information on the songs available in the application.

The database is well designed to properly record useful data and to optimize for queries on song play analysis, like how many users are free users, which song is more popular, etc...

## ETL pipeline

ETL is easy and straightforward in this project.

There are two `Python` to run:

1. `create_tables.py` which will first drop tables if they exist and then create all necessary tables in the sparkify database
2. `etl.py` which will populate database with the data coming from JSON files in the `data/log_data` and `data/song_data` folders

The file `sql_queries.py` is used to write and load when needed in another file all the SQL queries performed to drop or create the tables and run the insertions in the database.

So to run the ETL pipeline just run this two scripts in that order.

```
$ python create_tables.py
$ python etl.py
```

Jupyter Notebook `etl.ipynb` is used to build `etl.py` file by running some example before scaling out to all files in the folders.

Jupyter Notebook `test.ipynb` is used to be sure that we correctly inserted the data in the database.

## Example of queries

Some queries can be run as examples to illustrate database purpose for the startup and the analytics goals.

### Example 1

How many users are using the application with the free version and how many are using it with paid version?

```
SELECT level, COUNT(*)
FROM users
GROUP BY level
```

Result:

74 of the users are using the free version and only 22 are using it with the paid version.

### Example 2

What is the maximum number of songs per artist in the database and for which artists the number of songs is reach?

```
SELECT a.id, a.name, COUNT(*)
FROM songs AS s
JOIN artists AS a ON s.artist_id=a.id
WHERE a.id IN (
    SELECT a.id
    FROM songs AS s
    JOIN artists AS a ON s.artist_id=a.id
    GROUP BY a.id
    HAVING COUNT(*) = (SELECT MAX(nb) FROM (SELECT COUNT(*) AS nb FROM songs GROUP BY artist_id) AS t1)
    )
GROUP BY a.id
```

Result:

The maximum of songs for one artist is 2 songs and two artists have 2 songs in the database: Casual (id='ARD7TVE1187B99BFB1') and Clp (id='ARNTLGG11E2835DDB9').

### Example 3

What is the average duration of a song? And the maximum? And the minimum?

```
SELECT AVG(duration) AS mean, AVG(duration)/60 AS mean_minutes, MAX(duration) AS max, MAX(duration)/60 AS max_minutes, MIN(duration) AS min
FROM songs
```

Result:

The average duration for a song is 239.73 seconds (almost 4 minutes). The maximum is  599.249 seconds (almost 10 minutes). And the minimum is 29.544 seconds.
