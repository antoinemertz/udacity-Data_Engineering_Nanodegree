# PROJECT DATAWAREHOUSE ON AWS

## Introduction

A music streaming startup, Sparkify, has grown their user base and song database and want to move their processes and data onto the cloud. Their data resides in S3, in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

As their data engineer, I'm tasked with building an ETL pipeline that extracts their data from S3, stages them in Redshift, and transforms data into a set of dimensional tables for their analytics team to continue finding insights in what songs their users are listening to. I'll be able to test my database and ETL pipeline by running queries given to me by the analytics team from Sparkify and compare my results with their expected results.

## Description

In this project, you'll apply what you've learned on data warehouses and AWS to build an ETL pipeline for a database hosted on Redshift. To complete the project, you will need to load data from S3 to staging tables on Redshift and execute SQL statements that create the analytics tables from these staging tables.

## Project Datasets

I'll be working with two datasets that reside in S3. Here are the S3 links for each:

* song data: `s3://udacity-dend/song_data`
* log data: `s3://udacity-dend/log_data`

Log data json path: `s3://udacity-dend/log_json_path.json`

### Song Dataset

The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset.

```
song_data/A/B/C/TRABCEI128F424C983.json
song_data/A/A/B/TRAABJL12903CDCF1A.json
```

And below is an example of what a single song file, `TRAABJL12903CDCF1A.json`, looks like.

```
{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}
```

### Log Dataset
The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate app activity logs from an imaginary music streaming app based on configuration settings.

The log files in the dataset you'll be working with are partitioned by year and month. For example, here are filepaths to two files in this dataset.

```
log_data/2018/11/2018-11-12-events.json
log_data/2018/11/2018-11-13-events.json
```

## How to run the project

1. Configure tour projectby filling information on AWS account and AWS setup. Do it in the `dwh.cfg` file.

```
[AWS]
key    =
secret =

[REDSHIFT]
type          = multi-node
num_nodes     = 4
node_type     = dc2.large
iam_role_name = sparkify
identifier    = sparkifycluster

[CLUSTER]
endpoint    =
db_name     = sparkifydb
db_user     = sparkifyadmin
db_password = Spark1fy
db_port     = 5439


[IAM_ROLE]
arn =

[S3]
log_data = 's3://udacity-dend/log_data'
log_jsonpath = 's3://udacity-dend/log_json_path.json'
song_data = 's3://udacity-dend/song_data'
```

You just have to fill the `AWS` section with your AWS credentials. Next steps will take care of correctly fill Redshift cluster endpoint and ARN for IAM Role.

2. Run the AWS configuration Python script: it will create an IAM Role with correct policies and a Redshift cluster

`python config_aws_ressources.py`

3. Then, run the script to create tables in Redshift

`python create_tables.py`

4. Finally, run the ETL script to extract data from files in S3 buckets, to stage it in Redshift and then store it in the Data Warehouse in Redshift

`python etl.py`

## Project Structure

The project includes 4 Python scripts, one configuration file ans one README to sumup the project.

The 4 Python scripts are:

* `config_aws_ressources.py`: run to create IAM Role and Reshift cluster using Infrastructure As Code (IAC) as mindset
* `create_tables.py`: run to create staging tables and Data Warehouse ones in Redshift
* `etl.py`: run to extract S3 data to stage it in Redshift (staging tables) and populate Data Warehouse alos in Redshift

The `dwh.cfg` file is used to configure the project (AWS setup, Redshift cluster, IAM Role, ...). To be easy to use, it is just needed to fulfill the AWS credentials (KEY and SECRET). For IAM Role and cluster endpoint, `config_aws_ressources.py` script will be in charge of fulfilling the configuration parts.

The README file (the current one), is here to explain the project and detail how to use what I've done and how to run it.

### Database Schema (Redshift cluster)

#### Staging Tables

In term of database schema there is first two tables to stage the data coming from S3:

* `staging_events` table stores log data coming from the application of Sparkify (when someone is listening a music). It has the same structure as the log data (see Project Datasets part)
* `staging_songs` table stores song data coming from the extract of the One Million Songs (see Project Datasets part)

Then we describe the structure of the Datawarehouse: the data which will be used by the analytics team.

#### Fact Table

* `song_plays` - record event data associated with page `NextSong`  
    *songplay_id*  auto increment primary key  
    *start_time*   timestamp  
    *user_id*      integer  
    *level*        varchar  
    *song_id*      varchar  
    *artist_id*    varchar  
    *session_id*   varchar  
    *location*     varchar  
    *user_agent*   varchar  

#### Dimension Tables
* `uers` - store all app users
    *user_id*      integer pirmary key   
    *first_name*   varchar
    *last_name*    varchar
    *gender*       varchar
    *level*        varchar  

* `songs` - store all songs available in the app
    *song_id*      varchar primary key  
    *title*        varchar  
    *artist_id*    varchar  
    *year*         integer  
    *duration*     float  

* `artists` - store all artists availbale in the app
    *artist_id*    varchar primary key  
    *name*         varchar  
    *location*     varchar  
    *latitude*     float  
    *longitude*    float

* `time` - record all timestampswhen a music is listening in the app and extracted time statistics (hour, day, ...)
    *start_time*  timestamp primary key
    *hour*        integer
    *day*         integer
    *week*        integer
    *month*       integer
    *year*        integer
    *weekday*     varchar

#### Discussion

This schema is really basic. But it answers, I think, all necessary questions the analytics team can have.

First we have two staging tables to store raw data coming from S3 files. This staging area is used to have all raw data available before store it in a more appropriate way in the data warahouse with a fact table and dimensions tables. This staging area duplicates data from S3. But is is surely possible to decide to treat the data in batch (every day or week for example) and then delete data in the staging tables to only keep raw data in S3. We can safely delete data in the staging tables once the data are stored in the Data Warehouse because the raw data are coming from S3 and the IAM Role has only read access in S3 so data cannot be deleted in S3 buckets and raw data are always available.

Then the Data Warehouse has one fact table and then 4 dimension tables. The star schema is appropriate in our case because we don't need to break down lot of informations in the dimension tables (like city coming from counties coming from countries, ...). And with this schema, the analytics team has a clear view on how to request data and aggregate if needed. NO complexity here with basic SQL knowledges.

So I think I correctly answer the demand of the Sparkify team to build a Data Warehouse on AWS using the data store in S3 buckets.
